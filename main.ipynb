{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### cifar10数据集下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建文件夹\n",
    "# !mkdir -p  ./dataset/\n",
    "# 下载数据集\n",
    "# !wget \"http://ai-atest.bj.bcebos.com/cifar-10-python.tar.gz\" -O cifar-10-python.tar.gz\n",
    "# 移动\n",
    "# !mv cifar-10-python.tar.gz  ./dataset/\n",
    "# 查看\n",
    "# !ls -a ./dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import paddle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from paddle.static import InputSpec\n",
    "from paddle.regularizer import L2Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 定义网络结构， 使用模型为resnet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle import nn\n",
    "from paddle.vision.models.resnet import BasicBlock\n",
    "\n",
    "paddle.set_device('cpu')\n",
    "\n",
    "class ResNet(nn.Layer):\n",
    "    def __init__(self, block, depth, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        layer_cfg = {\n",
    "            20: [3, 3, 3],\n",
    "            32: [5, 5, 5],\n",
    "            44: [7, 7, 7],\n",
    "            56: [9, 9, 9],\n",
    "            110:[18, 18, 18],\n",
    "            1202:[200, 200, 200],\n",
    "        }\n",
    "        layers = layer_cfg[depth]\n",
    "        self.num_classes = num_classes\n",
    "        self._norm_layer = nn.BatchNorm2D\n",
    "\n",
    "        self.inplanes = 16\n",
    "        self.dilation = 1\n",
    "\n",
    "        self.conv1 = nn.Conv2D(\n",
    "            3,\n",
    "            self.inplanes,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias_attr=False)\n",
    "        self.bn1 = self._norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2D((1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2D(\n",
    "                    self.inplanes,\n",
    "                    planes * block.expansion,\n",
    "                    1,\n",
    "                    stride=stride,\n",
    "                    bias_attr=False),\n",
    "                norm_layer(planes * block.expansion), )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(self.inplanes, planes, stride, downsample, 1, 16,\n",
    "                  previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    # @paddle.jit.to_static(input_spec=[paddle.static.InputSpec(shape=[None, 3, 32, 32], name='x')])\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = paddle.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据处理 读取cifar10数据，采用数据增强手段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use GPU\n",
    "# place = paddle.CUDAPlace(0)\n",
    "# Use CPU if GPU is not available\n",
    "place = paddle.CPUPlace()\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据读取和数据增强\n",
    "import paddle\n",
    "import paddle.vision.transforms  as T\n",
    "from paddle.io import Dataset, BatchSampler, DataLoader\n",
    "\n",
    "# 使用transform对数据集做归一化\n",
    "print('download training data and load training data')\n",
    "#数据增强\n",
    "transform1=T.Compose(\n",
    "    [\n",
    "        T.RandomCrop(32, padding=4),# 按0.5的概率随机裁剪图片\n",
    "        T.RandomHorizontalFlip(0.5), # 按0.5的概率水平反转图片\n",
    "        T.Transpose(), # 格式转换\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) # 归一化\n",
    "    ]\n",
    ")\n",
    "transform2=T.Compose(\n",
    "    [\n",
    "        T.Transpose(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),# 归一化处理\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data = paddle.vision.datasets.cifar.Cifar10(\"./dataset/cifar-10-python.tar.gz\", mode=\"train\", transform=transform1)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=8,\n",
    "                 use_shared_memory=False)\n",
    "\n",
    "test_data = paddle.vision.datasets.cifar.Cifar10(\"./dataset/cifar-10-python.tar.gz\", mode=\"test\", transform=transform2)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2,\n",
    "                 use_shared_memory=False)\n",
    "print('load finished')\n",
    "\n",
    "print(\"训练集数量:{}, 测试集数量:{}\".format(len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义采集参数\n",
    "class AccLossCallback(paddle.callbacks.Callback):\n",
    "    # 高层api调回函数，定义采集参数\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.epoch_train_acc = []\n",
    "        self.epoch_train_loss = []\n",
    "        self.epoch_eval_acc = []\n",
    "        self.epoch_eval_loss = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # 训练过程中，每一轮结束调用一次\n",
    "        self.epoch_train_loss.append(logs.get('loss')[0])\n",
    "        self.epoch_train_acc.append(logs.get('acc'))\n",
    "    \n",
    "\n",
    "    def on_eval_end(self, logs=None):\n",
    "        # 每评估完成调用一次\n",
    "        self.epoch_eval_loss.append(logs.get('loss')[0])\n",
    "        self.epoch_eval_acc.append(logs.get('acc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ResNet(BasicBlock, 32)\n",
    "model = paddle.Model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 实例化参数记录器\n",
    "mylogs = AccLossCallback()\n",
    "# 学习率回调函数,每轮调用一次\n",
    "lr = paddle.callbacks.LRScheduler(by_step=False, by_epoch=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "val=[0.1,0.01,0.001]\n",
    "scheduler=paddle.optimizer.lr.PiecewiseDecay(boundaries=[80,120],values=val,verbose=False)\n",
    "#动量系数\n",
    "# momentum=0.1\n",
    "# momentum=0.5\n",
    "momentum=0.9\n",
    "\n",
    "#正则化系数\n",
    "# weight_decay=1e-1\n",
    "# weight_decay=1e-2\n",
    "# weight_decay=1e-3\n",
    "weight_decay=1e-4\n",
    "# weight_decay=1e-5\n",
    "\n",
    "optim =  paddle.optimizer.Momentum( parameters = model.parameters(), learning_rate = scheduler, momentum = momentum, weight_decay = weight_decay )\n",
    "# optim =  paddle.optimizer.Adam( parameters = model.parameters(), learning_rate = scheduler, weight_decay=L2Decay(weight_decay) )\n",
    "# optim =  paddle.optimizer.Adam( parameters = model.parameters(), learning_rate = scheduler)\n",
    "# optim =  paddle.optimizer.SGD( parameters = model.parameters(), learning_rate = scheduler,  weight_decay = weight_decay )\n",
    "# optim =  paddle.optimizer.Adagrad( parameters = model.parameters(), learning_rate = scheduler,  weight_decay = weight_decay )\n",
    "\n",
    "\n",
    "# 模型优化器，损失计算函数，准确率计算函数\n",
    "model.prepare(optim,\n",
    "              paddle.nn.CrossEntropyLoss(),\n",
    "              paddle.metric.Accuracy()\n",
    ")\n",
    "# 开始训练\n",
    "model.fit(train_loader,test_loader,epochs=150,batch_size=BATCH_SIZE,callbacks=[mylogs,lr],verbose=1,num_workers=4)\n",
    "# model.evaluate(test_loader,batch_size=BATCH_SIZE,callbacks=[mylogs,lr],verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 可视化模型训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 绘图\n",
    "def plot_loss_acc(mylogs):\n",
    "    plot_two_sup(title={\n",
    "        'suptitle': \"cifar-10 classification, best eval acc:{}\".format(np.max(mylogs.epoch_eval_acc[-1])),\n",
    "        'titles': [\"loss\", \"acc\"]\n",
    "    },\n",
    "        x_label=[\n",
    "            {\n",
    "                'label': \"epoch\",\n",
    "                'value': list(range(1,len(mylogs.epoch_eval_acc)+1))\n",
    "            },\n",
    "            {\n",
    "                'label': \"epoch\",\n",
    "                'value': list(range(1,len(mylogs.epoch_eval_loss)+1))\n",
    "            }\n",
    "        ],\n",
    "        y_label=[\n",
    "            {\n",
    "                'label': \"loss\",\n",
    "                'y_labels': [\n",
    "                    \"train_loss\",\n",
    "                    \"eval_loss\"\n",
    "                ],\n",
    "                'values': [\n",
    "                    mylogs.epoch_train_loss,\n",
    "                    mylogs.epoch_eval_loss\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                'label': \"acc\",\n",
    "                'y_labels': [\n",
    "                    \"train_acc\",\n",
    "                    \"eval_acc\"\n",
    "                ],\n",
    "                'values': [\n",
    "                    mylogs.epoch_train_acc,\n",
    "                    mylogs.epoch_eval_acc\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_two_sup(x_label=None, y_label=None, title=None):\n",
    "    \"\"\"\n",
    "    绘制双图，每个图的曲线数\n",
    "    :param title:　{\n",
    "        'subtitle':\"\"\n",
    "        \"titles':[\"\"]\n",
    "    }\n",
    "    :param x_label: 图的横坐标列表[\n",
    "        {\n",
    "            label:\n",
    "            value  [x1,x2,x3...]\n",
    "        }\n",
    "    ]\n",
    "    :param y_label: 图的纵坐标字典列表 [\n",
    "        {\n",
    "            label:\"\",\n",
    "            y_labels:[\n",
    "                \"label_1\",\n",
    "                \"label_2\",...\n",
    "            ]\n",
    "            values: [\n",
    "                [y1,y2,...]\n",
    "                [y1,y2,...]\n",
    "            ]\n",
    "\n",
    "        },...\n",
    "    ]\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.suptitle(title['suptitle'])\n",
    "    for index in range(2):\n",
    "        plt.subplot(1, 2, index + 1)\n",
    "        plt.title(title['titles'][index])\n",
    "        for i in range(len(y_label[index]['y_labels'])):\n",
    "            plt.plot(x_label[index]['value'], y_label[index]['values'][i], label=y_label[index]['y_labels'][i])\n",
    "        plt.xlabel(x_label[index]['label'])\n",
    "        plt.ylabel(y_label[index]['label'])\n",
    "        plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(mylogs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
