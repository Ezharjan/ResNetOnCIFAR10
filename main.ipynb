{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### cifar10数据集下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建文件夹\n",
    "# !mkdir -p  /home/aistudio/.cache/paddle/dataset/cifar/ \n",
    "# 下载数据集\n",
    "# !wget \"http://ai-atest.bj.bcebos.com/cifar-10-python.tar.gz\" -O cifar-10-python.tar.gz\n",
    "# 移动\n",
    "# !mv cifar-10-python.tar.gz  /home/aistudio/.cache/paddle/dataset/cifar/\n",
    "# 查看\n",
    "# !ls -a /home/aistudio/.cache/paddle/dataset/cifar/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import paddle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from paddle.static import InputSpec\n",
    "from paddle.regularizer import L2Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 定义网络结构， 使用模型为resnet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle import nn\n",
    "from paddle.vision.models.resnet import BasicBlock\n",
    "\n",
    "\n",
    "class ResNet(nn.Layer):\n",
    "    def __init__(self, block, depth, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        layer_cfg = {\n",
    "            20: [3, 3, 3],\n",
    "            32: [5, 5, 5],\n",
    "            44: [7, 7, 7],\n",
    "            56: [9, 9, 9],\n",
    "            110:[18, 18, 18],\n",
    "            1202:[200, 200, 200],\n",
    "        }\n",
    "        layers = layer_cfg[depth]\n",
    "        self.num_classes = num_classes\n",
    "        self._norm_layer = nn.BatchNorm2D\n",
    "\n",
    "        self.inplanes = 16\n",
    "        self.dilation = 1\n",
    "\n",
    "        self.conv1 = nn.Conv2D(\n",
    "            3,\n",
    "            self.inplanes,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias_attr=False)\n",
    "        self.bn1 = self._norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2D((1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2D(\n",
    "                    self.inplanes,\n",
    "                    planes * block.expansion,\n",
    "                    1,\n",
    "                    stride=stride,\n",
    "                    bias_attr=False),\n",
    "                norm_layer(planes * block.expansion), )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(self.inplanes, planes, stride, downsample, 1, 16,\n",
    "                  previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    # @paddle.jit.to_static(input_spec=[paddle.static.InputSpec(shape=[None, 3, 32, 32], name='x')])\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = paddle.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据处理 读取cifar10数据，采用数据增强手段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "place = paddle.CUDAPlace(0)\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download training data and load training data\n",
      "load finished\n",
      "训练集数量:50000, 测试集数量:10000\n"
     ]
    }
   ],
   "source": [
    "# 数据读取和数据增强\n",
    "import paddle\n",
    "import paddle.vision.transforms  as T\n",
    "from paddle.io import Dataset, BatchSampler, DataLoader\n",
    "\n",
    "# 使用transform对数据集做归一化\n",
    "print('download training data and load training data')\n",
    "#数据增强\n",
    "transform1=T.Compose(\n",
    "    [\n",
    "        T.RandomCrop(32, padding=4),# 按0.5的概率随机裁剪图片\n",
    "        T.RandomHorizontalFlip(0.5), # 按0.5的概率水平反转图片\n",
    "        T.Transpose(), # 格式转换\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) # 归一化\n",
    "    ]\n",
    ")\n",
    "transform2=T.Compose(\n",
    "    [\n",
    "        T.Transpose(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),# 归一化处理\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data = paddle.vision.datasets.cifar.Cifar10(\"./cifar-10-python.tar.gz\", mode=\"train\", transform=transform1)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=8,\n",
    "                 use_shared_memory=False)\n",
    "\n",
    "test_data = paddle.vision.datasets.cifar.Cifar10(\"./cifar-10-python.tar.gz\", mode=\"test\", transform=transform2)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2,\n",
    "                 use_shared_memory=False)\n",
    "print('load finished')\n",
    "\n",
    "print(\"训练集数量:{}, 测试集数量:{}\".format(len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义采集参数\n",
    "class AccLossCallback(paddle.callbacks.Callback):\n",
    "    # 飞浆高层api调回函数，定义采集参数\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.epoch_train_acc = []\n",
    "        self.epoch_train_loss = []\n",
    "        self.epoch_eval_acc = []\n",
    "        self.epoch_eval_loss = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # 训练过程中，每一轮结束调用一次\n",
    "        self.epoch_train_loss.append(logs.get('loss')[0])\n",
    "        self.epoch_train_acc.append(logs.get('acc'))\n",
    "    \n",
    "\n",
    "    def on_eval_end(self, logs=None):\n",
    "        # 每评估完成调用一次\n",
    "        self.epoch_eval_loss.append(logs.get('loss')[0])\n",
    "        self.epoch_eval_acc.append(logs.get('acc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ResNet(BasicBlock, 32)\n",
    "model = paddle.Model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 实例化参数记录器\r\n",
    "mylogs = AccLossCallback()\r\n",
    "# 学习率回调函数,每轮调用一次\r\n",
    "lr = paddle.callbacks.LRScheduler(by_step=False, by_epoch=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous step.\n",
      "Epoch 1/150\n",
      "step 98/98 [==============================] - loss: 2.0906 - acc: 0.1563 - 97ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 2.0932 - acc: 0.1806 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 2/150\n",
      "step 98/98 [==============================] - loss: 1.6988 - acc: 0.2804 - 97ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.7956 - acc: 0.3414 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 3/150\n",
      "step 98/98 [==============================] - loss: 1.4725 - acc: 0.3939 - 97ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.5865 - acc: 0.4300 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 4/150\n",
      "step 98/98 [==============================] - loss: 1.2630 - acc: 0.4872 - 97ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.4397 - acc: 0.5029 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 5/150\n",
      "step 98/98 [==============================] - loss: 1.1873 - acc: 0.5723 - 119ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.4025 - acc: 0.5414 - 81ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 6/150\n",
      "step 98/98 [==============================] - loss: 0.8773 - acc: 0.6417 - 97ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2162 - acc: 0.6249 - 82ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 7/150\n",
      "step 98/98 [==============================] - loss: 0.8568 - acc: 0.7007 - 98ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1371 - acc: 0.6627 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 8/150\n",
      "step 98/98 [==============================] - loss: 0.6852 - acc: 0.7417 - 98ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2610 - acc: 0.6717 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 9/150\n",
      "step 98/98 [==============================] - loss: 0.6244 - acc: 0.7738 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.0739 - acc: 0.7024 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 10/150\n",
      "step 98/98 [==============================] - loss: 0.5504 - acc: 0.8014 - 103ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1188 - acc: 0.6771 - 98ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 11/150\n",
      "step 98/98 [==============================] - loss: 0.5110 - acc: 0.8255 - 111ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.0402 - acc: 0.7219 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 12/150\n",
      "step 98/98 [==============================] - loss: 0.5619 - acc: 0.8469 - 98ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.0486 - acc: 0.7366 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 13/150\n",
      "step 98/98 [==============================] - loss: 0.3883 - acc: 0.8635 - 99ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 0.9984 - acc: 0.7232 - 81ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 14/150\n",
      "step 98/98 [==============================] - loss: 0.4021 - acc: 0.8836 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1531 - acc: 0.7077 - 80ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 15/150\n",
      "step 98/98 [==============================] - loss: 0.3563 - acc: 0.8949 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2909 - acc: 0.7186 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 16/150\n",
      "step 98/98 [==============================] - loss: 0.2469 - acc: 0.9102 - 117ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.7290 - acc: 0.6620 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 17/150\n",
      "step 98/98 [==============================] - loss: 0.2878 - acc: 0.9150 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1661 - acc: 0.7364 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 18/150\n",
      "step 98/98 [==============================] - loss: 0.2492 - acc: 0.9297 - 101ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1433 - acc: 0.7352 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 19/150\n",
      "step 98/98 [==============================] - loss: 0.2215 - acc: 0.9433 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.3925 - acc: 0.7260 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 20/150\n",
      "step 98/98 [==============================] - loss: 0.1281 - acc: 0.9466 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2914 - acc: 0.7579 - 82ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 21/150\n",
      "step 98/98 [==============================] - loss: 0.2143 - acc: 0.9438 - 104ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.9647 - acc: 0.6374 - 89ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 22/150\n",
      "step 98/98 [==============================] - loss: 0.1748 - acc: 0.9538 - 109ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.4990 - acc: 0.7219 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 23/150\n",
      "step 98/98 [==============================] - loss: 0.1070 - acc: 0.9603 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.3557 - acc: 0.7586 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 24/150\n",
      "step 98/98 [==============================] - loss: 0.1592 - acc: 0.9632 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.6623 - acc: 0.7192 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 25/150\n",
      "step 98/98 [==============================] - loss: 0.1364 - acc: 0.9663 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2619 - acc: 0.7567 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 26/150\n",
      "step 98/98 [==============================] - loss: 0.1025 - acc: 0.9707 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.5062 - acc: 0.7422 - 89ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 27/150\n",
      "step 98/98 [==============================] - loss: 0.0641 - acc: 0.9768 - 122ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.4611 - acc: 0.7406 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 28/150\n",
      "step 98/98 [==============================] - loss: 0.0914 - acc: 0.9769 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.4676 - acc: 0.7519 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 29/150\n",
      "step 98/98 [==============================] - loss: 0.1827 - acc: 0.9714 - 101ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.9301 - acc: 0.7203 - 88ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 30/150\n",
      "step 98/98 [==============================] - loss: 0.0840 - acc: 0.9772 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.5748 - acc: 0.7502 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 31/150\n",
      "step 98/98 [==============================] - loss: 0.0616 - acc: 0.9810 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2858 - acc: 0.7673 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 32/150\n",
      "step 98/98 [==============================] - loss: 0.0788 - acc: 0.9810 - 109ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.3394 - acc: 0.7608 - 100ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 33/150\n",
      "step 98/98 [==============================] - loss: 0.0646 - acc: 0.9820 - 107ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.6156 - acc: 0.7534 - 83ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 34/150\n",
      "step 98/98 [==============================] - loss: 0.0419 - acc: 0.9833 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.5941 - acc: 0.7746 - 80ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 35/150\n",
      "step 98/98 [==============================] - loss: 0.0724 - acc: 0.9818 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.5024 - acc: 0.7613 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 36/150\n",
      "step 98/98 [==============================] - loss: 0.0394 - acc: 0.9851 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.7917 - acc: 0.7363 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 37/150\n",
      "step 98/98 [==============================] - loss: 0.0899 - acc: 0.9847 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.3782 - acc: 0.7525 - 91ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 38/150\n",
      "step 98/98 [==============================] - loss: 0.0404 - acc: 0.9838 - 118ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.6837 - acc: 0.7606 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 39/150\n",
      "step 98/98 [==============================] - loss: 0.0604 - acc: 0.9849 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.4837 - acc: 0.7576 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 40/150\n",
      "step 98/98 [==============================] - loss: 0.0551 - acc: 0.9861 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.7851 - acc: 0.7525 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 41/150\n",
      "step 98/98 [==============================] - loss: 0.0340 - acc: 0.9904 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.5657 - acc: 0.7649 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 42/150\n",
      "step 98/98 [==============================] - loss: 0.0249 - acc: 0.9879 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.6426 - acc: 0.7465 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 43/150\n",
      "step 98/98 [==============================] - loss: 0.0326 - acc: 0.9880 - 111ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.5926 - acc: 0.7530 - 106ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 44/150\n",
      "step 98/98 [==============================] - loss: 0.0569 - acc: 0.9871 - 103ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.4612 - acc: 0.7622 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 45/150\n",
      "step 98/98 [==============================] - loss: 0.0642 - acc: 0.9871 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.4154 - acc: 0.7533 - 90ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 46/150\n",
      "step 98/98 [==============================] - loss: 0.0569 - acc: 0.9861 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.6209 - acc: 0.7481 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 47/150\n",
      "step 98/98 [==============================] - loss: 0.0786 - acc: 0.9818 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.6958 - acc: 0.7549 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 48/150\n",
      "step 98/98 [==============================] - loss: 0.0728 - acc: 0.9815 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2673 - acc: 0.7652 - 102ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 49/150\n",
      "step 98/98 [==============================] - loss: 0.0403 - acc: 0.9828 - 117ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.6928 - acc: 0.7579 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 50/150\n",
      "step 98/98 [==============================] - loss: 0.0527 - acc: 0.9869 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.6002 - acc: 0.7497 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 51/150\n",
      "step 98/98 [==============================] - loss: 0.0445 - acc: 0.9898 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.6574 - acc: 0.7536 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 52/150\n",
      "step 98/98 [==============================] - loss: 0.0238 - acc: 0.9900 - 104ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.5286 - acc: 0.7681 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 53/150\n",
      "step 98/98 [==============================] - loss: 0.0294 - acc: 0.9915 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.4312 - acc: 0.7731 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 54/150\n",
      "step 98/98 [==============================] - loss: 0.0650 - acc: 0.9917 - 114ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.5425 - acc: 0.7664 - 101ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 55/150\n",
      "step 98/98 [==============================] - loss: 0.0116 - acc: 0.9910 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.5873 - acc: 0.7701 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 56/150\n",
      "step 98/98 [==============================] - loss: 0.0484 - acc: 0.9870 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.5575 - acc: 0.7354 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 57/150\n",
      "step 98/98 [==============================] - loss: 0.0421 - acc: 0.9827 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.9032 - acc: 0.7257 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 58/150\n",
      "step 98/98 [==============================] - loss: 0.0359 - acc: 0.9895 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.7982 - acc: 0.7565 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 59/150\n",
      "step 98/98 [==============================] - loss: 0.0277 - acc: 0.9924 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.5179 - acc: 0.7517 - 105ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 60/150\n",
      "step 98/98 [==============================] - loss: 0.0308 - acc: 0.9945 - 114ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.5399 - acc: 0.7708 - 87ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 61/150\n",
      "step 98/98 [==============================] - loss: 0.0077 - acc: 0.9965 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.4826 - acc: 0.7797 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 62/150\n",
      "step 98/98 [==============================] - loss: 0.0107 - acc: 0.9972 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.3747 - acc: 0.7813 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 63/150\n",
      "step 98/98 [==============================] - loss: 0.0048 - acc: 0.9989 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.3866 - acc: 0.7959 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 64/150\n",
      "step 98/98 [==============================] - loss: 0.0014 - acc: 0.9997 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.3165 - acc: 0.8012 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 65/150\n",
      "step 98/98 [==============================] - loss: 6.6392e-04 - acc: 1.0000 - 120ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.3148 - acc: 0.8021 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 66/150\n",
      "step 98/98 [==============================] - loss: 4.3149e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2942 - acc: 0.8026 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 67/150\n",
      "step 98/98 [==============================] - loss: 4.1014e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2954 - acc: 0.8050 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 68/150\n",
      "step 98/98 [==============================] - loss: 4.0697e-04 - acc: 1.0000 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2814 - acc: 0.8029 - 88ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 69/150\n",
      "step 98/98 [==============================] - loss: 4.6889e-04 - acc: 1.0000 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2612 - acc: 0.8046 - 87ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 70/150\n",
      "step 98/98 [==============================] - loss: 3.0888e-04 - acc: 1.0000 - 104ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2464 - acc: 0.8045 - 110ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 71/150\n",
      "step 98/98 [==============================] - loss: 5.2211e-04 - acc: 1.0000 - 109ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2392 - acc: 0.8052 - 88ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 72/150\n",
      "step 98/98 [==============================] - loss: 3.2957e-04 - acc: 1.0000 - 103ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2488 - acc: 0.8045 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 73/150\n",
      "step 98/98 [==============================] - loss: 3.7577e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2296 - acc: 0.8037 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 74/150\n",
      "step 98/98 [==============================] - loss: 3.0224e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2164 - acc: 0.8038 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 75/150\n",
      "step 98/98 [==============================] - loss: 5.4985e-04 - acc: 1.0000 - 103ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.2119 - acc: 0.8040 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 76/150\n",
      "step 98/98 [==============================] - loss: 4.4523e-04 - acc: 1.0000 - 119ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1993 - acc: 0.8044 - 82ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 77/150\n",
      "step 98/98 [==============================] - loss: 3.0023e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1964 - acc: 0.8032 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 78/150\n",
      "step 98/98 [==============================] - loss: 3.9286e-04 - acc: 1.0000 - 98ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1789 - acc: 0.8034 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 79/150\n",
      "step 98/98 [==============================] - loss: 3.6237e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1720 - acc: 0.8035 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 80/150\n",
      "step 98/98 [==============================] - loss: 6.2623e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1653 - acc: 0.8027 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 81/150\n",
      "step 98/98 [==============================] - loss: 4.4057e-04 - acc: 1.0000 - 107ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1712 - acc: 0.8030 - 106ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 82/150\n",
      "step 98/98 [==============================] - loss: 3.9698e-04 - acc: 1.0000 - 104ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1729 - acc: 0.8019 - 87ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 83/150\n",
      "step 98/98 [==============================] - loss: 3.4787e-04 - acc: 1.0000 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1789 - acc: 0.8016 - 87ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 84/150\n",
      "step 98/98 [==============================] - loss: 4.7642e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1753 - acc: 0.8024 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 85/150\n",
      "step 98/98 [==============================] - loss: 2.8926e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1650 - acc: 0.8019 - 88ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 86/150\n",
      "step 98/98 [==============================] - loss: 3.2426e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1591 - acc: 0.8022 - 87ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 87/150\n",
      "step 98/98 [==============================] - loss: 4.0835e-04 - acc: 1.0000 - 120ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1591 - acc: 0.8025 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 88/150\n",
      "step 98/98 [==============================] - loss: 4.7244e-04 - acc: 1.0000 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1579 - acc: 0.8024 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 89/150\n",
      "step 98/98 [==============================] - loss: 5.4371e-04 - acc: 1.0000 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1602 - acc: 0.8034 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 90/150\n",
      "step 98/98 [==============================] - loss: 2.8985e-04 - acc: 1.0000 - 101ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1651 - acc: 0.8022 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 91/150\n",
      "step 98/98 [==============================] - loss: 6.7784e-04 - acc: 1.0000 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1662 - acc: 0.8020 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 92/150\n",
      "step 98/98 [==============================] - loss: 4.1423e-04 - acc: 1.0000 - 109ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1627 - acc: 0.8018 - 105ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 93/150\n",
      "step 98/98 [==============================] - loss: 4.7763e-04 - acc: 1.0000 - 104ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1590 - acc: 0.8029 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 94/150\n",
      "step 98/98 [==============================] - loss: 4.7126e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1578 - acc: 0.8018 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 95/150\n",
      "step 98/98 [==============================] - loss: 3.4188e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1598 - acc: 0.8012 - 87ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 96/150\n",
      "step 98/98 [==============================] - loss: 3.5103e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1624 - acc: 0.8008 - 88ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 97/150\n",
      "step 98/98 [==============================] - loss: 3.3864e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1583 - acc: 0.8025 - 100ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 98/150\n",
      "step 98/98 [==============================] - loss: 4.0605e-04 - acc: 1.0000 - 118ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1611 - acc: 0.8019 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 99/150\n",
      "step 98/98 [==============================] - loss: 3.5798e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1542 - acc: 0.8015 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 100/150\n",
      "step 98/98 [==============================] - loss: 4.0660e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1591 - acc: 0.8010 - 89ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 101/150\n",
      "step 98/98 [==============================] - loss: 4.5300e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1616 - acc: 0.8010 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 102/150\n",
      "step 98/98 [==============================] - loss: 4.0979e-04 - acc: 1.0000 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1486 - acc: 0.8019 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 103/150\n",
      "step 98/98 [==============================] - loss: 6.0785e-04 - acc: 1.0000 - 111ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1524 - acc: 0.8008 - 112ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 104/150\n",
      "step 98/98 [==============================] - loss: 3.5037e-04 - acc: 1.0000 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1451 - acc: 0.8014 - 82ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 105/150\n",
      "step 98/98 [==============================] - loss: 4.4705e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1516 - acc: 0.8007 - 88ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 106/150\n",
      "step 98/98 [==============================] - loss: 7.6710e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1491 - acc: 0.8018 - 87ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 107/150\n",
      "step 98/98 [==============================] - loss: 3.4790e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1488 - acc: 0.8009 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 108/150\n",
      "step 98/98 [==============================] - loss: 4.0179e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1469 - acc: 0.8013 - 96ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 109/150\n",
      "step 98/98 [==============================] - loss: 4.7123e-04 - acc: 1.0000 - 119ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1362 - acc: 0.8023 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 110/150\n",
      "step 98/98 [==============================] - loss: 3.8628e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1452 - acc: 0.8016 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 111/150\n",
      "step 98/98 [==============================] - loss: 3.0097e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1418 - acc: 0.8012 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 112/150\n",
      "step 98/98 [==============================] - loss: 4.5681e-04 - acc: 1.0000 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1470 - acc: 0.8016 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 113/150\n",
      "step 98/98 [==============================] - loss: 5.2265e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1566 - acc: 0.8013 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 114/150\n",
      "step 98/98 [==============================] - loss: 3.5806e-04 - acc: 1.0000 - 112ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1448 - acc: 0.8002 - 99ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 115/150\n",
      "step 98/98 [==============================] - loss: 3.3164e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1466 - acc: 0.8009 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 116/150\n",
      "step 98/98 [==============================] - loss: 7.6085e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1476 - acc: 0.8013 - 87ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 117/150\n",
      "step 98/98 [==============================] - loss: 4.4325e-04 - acc: 1.0000 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1450 - acc: 0.8013 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 118/150\n",
      "step 98/98 [==============================] - loss: 2.9807e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1451 - acc: 0.8012 - 88ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 119/150\n",
      "step 98/98 [==============================] - loss: 3.1954e-04 - acc: 1.0000 - 99ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1405 - acc: 0.8010 - 83ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 120/150\n",
      "step 98/98 [==============================] - loss: 4.0023e-04 - acc: 1.0000 - 120ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1444 - acc: 0.8011 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 121/150\n",
      "step 98/98 [==============================] - loss: 3.5922e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1413 - acc: 0.8002 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 122/150\n",
      "step 98/98 [==============================] - loss: 3.7747e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1420 - acc: 0.8010 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 123/150\n",
      "step 98/98 [==============================] - loss: 3.6864e-04 - acc: 1.0000 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1425 - acc: 0.8017 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 124/150\n",
      "step 98/98 [==============================] - loss: 4.3327e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1479 - acc: 0.8013 - 82ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 125/150\n",
      "step 98/98 [==============================] - loss: 4.3824e-04 - acc: 1.0000 - 115ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1463 - acc: 0.8010 - 89ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 126/150\n",
      "step 98/98 [==============================] - loss: 5.4700e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1382 - acc: 0.8014 - 90ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 127/150\n",
      "step 98/98 [==============================] - loss: 4.9602e-04 - acc: 1.0000 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1509 - acc: 0.8018 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 128/150\n",
      "step 98/98 [==============================] - loss: 4.0546e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1375 - acc: 0.8004 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 129/150\n",
      "step 98/98 [==============================] - loss: 3.1484e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1461 - acc: 0.8008 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 130/150\n",
      "step 98/98 [==============================] - loss: 3.6286e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1379 - acc: 0.8011 - 91ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 131/150\n",
      "step 98/98 [==============================] - loss: 4.8501e-04 - acc: 1.0000 - 120ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1411 - acc: 0.8016 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 132/150\n",
      "step 98/98 [==============================] - loss: 5.6128e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1486 - acc: 0.8006 - 87ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 133/150\n",
      "step 98/98 [==============================] - loss: 4.4089e-04 - acc: 1.0000 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1430 - acc: 0.8010 - 88ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 134/150\n",
      "step 98/98 [==============================] - loss: 3.1972e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1447 - acc: 0.8007 - 87ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 135/150\n",
      "step 98/98 [==============================] - loss: 7.9604e-04 - acc: 1.0000 - 100ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1457 - acc: 0.8013 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 136/150\n",
      "step 98/98 [==============================] - loss: 4.3432e-04 - acc: 1.0000 - 115ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1412 - acc: 0.8005 - 91ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 137/150\n",
      "step 98/98 [==============================] - loss: 2.9589e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1366 - acc: 0.8004 - 86ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 138/150\n",
      "step 98/98 [==============================] - loss: 4.3936e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1352 - acc: 0.8011 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 139/150\n",
      "step 98/98 [==============================] - loss: 4.2959e-04 - acc: 1.0000 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1361 - acc: 0.8018 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 140/150\n",
      "step 98/98 [==============================] - loss: 5.9462e-04 - acc: 1.0000 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1426 - acc: 0.8011 - 88ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 141/150\n",
      "step 98/98 [==============================] - loss: 3.9270e-04 - acc: 1.0000 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1431 - acc: 0.8012 - 95ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 142/150\n",
      "step 98/98 [==============================] - loss: 4.3712e-04 - acc: 1.0000 - 119ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1420 - acc: 0.8013 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 143/150\n",
      "step 98/98 [==============================] - loss: 3.7080e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1434 - acc: 0.8008 - 82ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 144/150\n",
      "step 98/98 [==============================] - loss: 4.1418e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1481 - acc: 0.8018 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 145/150\n",
      "step 98/98 [==============================] - loss: 4.0884e-04 - acc: 1.0000 - 101ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1465 - acc: 0.8010 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 146/150\n",
      "step 98/98 [==============================] - loss: 3.9902e-04 - acc: 1.0000 - 101ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1405 - acc: 0.8004 - 84ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 147/150\n",
      "step 98/98 [==============================] - loss: 3.9222e-04 - acc: 1.0000 - 122ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1445 - acc: 0.8009 - 88ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 148/150\n",
      "step 98/98 [==============================] - loss: 3.9980e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1518 - acc: 0.8013 - 83ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 149/150\n",
      "step 98/98 [==============================] - loss: 4.4807e-04 - acc: 1.0000 - 102ms/step         \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1423 - acc: 0.8018 - 85ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 150/150\n",
      "step 98/98 [==============================] - loss: 3.1697e-04 - acc: 1.0000 - 101ms/step        \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 20/20 [==============================] - loss: 1.1416 - acc: 0.8013 - 83ms/step          \n",
      "Eval samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# 定义优化器\n",
    "val=[0.1,0.01,0.001]\n",
    "scheduler=paddle.optimizer.lr.PiecewiseDecay(boundaries=[80,120],values=val,verbose=False)\n",
    "#动量系数\n",
    "# momentum=0.1\n",
    "# momentum=0.5\n",
    "momentum=0.9\n",
    "\n",
    "#正则化系数\n",
    "# weight_decay=1e-1\n",
    "# weight_decay=1e-2\n",
    "# weight_decay=1e-3\n",
    "weight_decay=1e-4\n",
    "# weight_decay=1e-5\n",
    "\n",
    "optim =  paddle.optimizer.Momentum( parameters = model.parameters(), learning_rate = scheduler, momentum = momentum, weight_decay = weight_decay )\n",
    "# optim =  paddle.optimizer.Adam( parameters = model.parameters(), learning_rate = scheduler, weight_decay=L2Decay(weight_decay) )\n",
    "# optim =  paddle.optimizer.Adam( parameters = model.parameters(), learning_rate = scheduler)\n",
    "# optim =  paddle.optimizer.SGD( parameters = model.parameters(), learning_rate = scheduler,  weight_decay = weight_decay )\n",
    "# optim =  paddle.optimizer.Adagrad( parameters = model.parameters(), learning_rate = scheduler,  weight_decay = weight_decay )\n",
    "\n",
    "\n",
    "# 模型优化器，损失计算函数，准确率计算函数\n",
    "model.prepare(optim,\n",
    "              paddle.nn.CrossEntropyLoss(),\n",
    "              paddle.metric.Accuracy()\n",
    ")\n",
    "# 开始训练\n",
    "model.fit(train_loader,test_loader,epochs=150,batch_size=BATCH_SIZE,callbacks=[mylogs,lr],verbose=1,num_workers=4)\n",
    "# model.evaluate(test_loader,batch_size=BATCH_SIZE,callbacks=[mylogs,lr],verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 可视化模型训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 绘图\r\n",
    "def plot_loss_acc(mylogs):\r\n",
    "    plot_two_sup(title={\r\n",
    "        'suptitle': \"cifar-10 classification, best eval acc:{}\".format(np.max(mylogs.epoch_eval_acc[-1])),\r\n",
    "        'titles': [\"loss\", \"acc\"]\r\n",
    "    },\r\n",
    "        x_label=[\r\n",
    "            {\r\n",
    "                'label': \"epoch\",\r\n",
    "                'value': list(range(1,len(mylogs.epoch_eval_acc)+1))\r\n",
    "            },\r\n",
    "            {\r\n",
    "                'label': \"epoch\",\r\n",
    "                'value': list(range(1,len(mylogs.epoch_eval_loss)+1))\r\n",
    "            }\r\n",
    "        ],\r\n",
    "        y_label=[\r\n",
    "            {\r\n",
    "                'label': \"loss\",\r\n",
    "                'y_labels': [\r\n",
    "                    \"train_loss\",\r\n",
    "                    \"eval_loss\"\r\n",
    "                ],\r\n",
    "                'values': [\r\n",
    "                    mylogs.epoch_train_loss,\r\n",
    "                    mylogs.epoch_eval_loss\r\n",
    "                ]\r\n",
    "            },\r\n",
    "            {\r\n",
    "                'label': \"acc\",\r\n",
    "                'y_labels': [\r\n",
    "                    \"train_acc\",\r\n",
    "                    \"eval_acc\"\r\n",
    "                ],\r\n",
    "                'values': [\r\n",
    "                    mylogs.epoch_train_acc,\r\n",
    "                    mylogs.epoch_eval_acc\r\n",
    "                ]\r\n",
    "            }\r\n",
    "        ]\r\n",
    "    )\r\n",
    "\r\n",
    "\r\n",
    "def plot_two_sup(x_label=None, y_label=None, title=None):\r\n",
    "    \"\"\"\r\n",
    "    绘制双图，每个图的曲线数\r\n",
    "    :param title:　{\r\n",
    "        'subtitle':\"\"\r\n",
    "        \"titles':[\"\"]\r\n",
    "    }\r\n",
    "    :param x_label: 图的横坐标列表[\r\n",
    "        {\r\n",
    "            label:\r\n",
    "            value  [x1,x2,x3...]\r\n",
    "        }\r\n",
    "    ]\r\n",
    "    :param y_label: 图的纵坐标字典列表 [\r\n",
    "        {\r\n",
    "            label:\"\",\r\n",
    "            y_labels:[\r\n",
    "                \"label_1\",\r\n",
    "                \"label_2\",...\r\n",
    "            ]\r\n",
    "            values: [\r\n",
    "                [y1,y2,...]\r\n",
    "                [y1,y2,...]\r\n",
    "            ]\r\n",
    "\r\n",
    "        },...\r\n",
    "    ]\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    plt.figure(figsize=(8,4))\r\n",
    "    plt.suptitle(title['suptitle'])\r\n",
    "    for index in range(2):\r\n",
    "        plt.subplot(1, 2, index + 1)\r\n",
    "        plt.title(title['titles'][index])\r\n",
    "        for i in range(len(y_label[index]['y_labels'])):\r\n",
    "            plt.plot(x_label[index]['value'], y_label[index]['values'][i], label=y_label[index]['y_labels'][i])\r\n",
    "        plt.xlabel(x_label[index]['label'])\r\n",
    "        plt.ylabel(y_label[index]['label'])\r\n",
    "        plt.legend()\r\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_acc(mylogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
